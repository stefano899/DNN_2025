{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-21T11:07:29.518202Z",
     "start_time": "2025-05-21T11:07:25.782457Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.io import read_image\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# VERIFICA PRESENZA DI CUDA\n",
    "Con nvidia-smi nel prompt si vede anche la versione di Cuda"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1416a182ed7008e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-21T11:07:29.998706Z",
     "start_time": "2025-05-21T11:07:29.519351Z"
    }
   },
   "id": "6c5a904dedfbd056",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DOWNLOADING DATASET"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ec29b000effee7c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "train_data= datasets.FashionMNIST(root='data', train=True, download=True, transform=ToTensor(),)\n",
    "\n",
    "test_data = datasets.FashionMNIST(root='data', train=False, download=True, transform=ToTensor(),)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-21T11:07:42.793989Z",
     "start_time": "2025-05-21T11:07:29.999628Z"
    }
   },
   "id": "270b3c042025f252",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DATA LOADER"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "943285fe8adc2eb5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "\n",
    "def handle_dataset():\n",
    "    train_data = datasets.FashionMNIST(root='data', train=True, download=True, transform=ToTensor(), )\n",
    "    test_data = datasets.FashionMNIST(root='data', train=False, download=True, transform=ToTensor(), )\n",
    "\n",
    "    labels_map = { # Classes need to be predicted\n",
    "        0: 'T-shirt',\n",
    "        1: 'Trouser',\n",
    "        2: 'Pullover',\n",
    "        3: 'Dress',\n",
    "        4: 'Coat',\n",
    "        5: 'Sandal',\n",
    "        6: 'Shirt',\n",
    "        7: 'Sneaker',\n",
    "        8: 'Bag',\n",
    "        9: 'Ankle Boot',\n",
    "    }\n",
    "    batch_size = 128  # For processing simultaneously 128 images at every weigth update\n",
    "\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size,\n",
    "                                  shuffle=True)  # For every iteration, dataset is divided into gropus of 128 samples. Shuffle helps generalizing the model\n",
    "\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size)  # Same as train_dataloader but for the test\n",
    "\n",
    "    return train_dataloader, test_dataloader, labels_map\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-21T11:07:42.799058Z",
     "start_time": "2025-05-21T11:07:42.795521Z"
    }
   },
   "id": "dc932b3fd1b04c00",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DEFYINING THE INITIALIZATION KERNEL AND WEIGHTS\n",
    "We'll have 1 input channel, since the images are in grayscale, and we select By hand the kernel size, which in this case is in the size of 4 (heights and width are multiple of 4)\n",
    "\n",
    "For understanding how will be the size of the image after applying a convolution layer, we have to see this formula:\n",
    "$$O = \\frac{(I - K + 2P)}{S} + 1$$\n",
    "\n",
    "Where *I* is the size of the input, *K* is the size of the kernel, *P* is the padding and *S* is the stride\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f539427361a4ac9"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m test_convolutional_layer = nn.Conv2d(in_channels=\u001B[32m1\u001B[39m, out_channels=\u001B[32m5\u001B[39m, kernel_size=\u001B[32m3\u001B[39m, padding=\u001B[32m1\u001B[39m) \u001B[38;5;66;03m# Defines our kernel for our first layer of convolution. \u001B[39;00m\n\u001B[32m      2\u001B[39m test_max_pool = nn.MaxPool2d(kernel_size=\u001B[32m2\u001B[39m, stride=\u001B[32m2\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m x = \u001B[38;5;28mnext\u001B[39m(train_dataloader.\u001B[34m__iter__\u001B[39m())[\u001B[32m0\u001B[39m]\n\u001B[32m      5\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m'\u001B[39m\u001B[33mShape of x:\u001B[39m\u001B[33m'\u001B[39m, x.shape)\n\u001B[32m      6\u001B[39m \u001B[38;5;66;03m## The shape will be 128, 1, 28, 28 because we have 128 images in a batch, 1 channel, and 28x28 pixels\u001B[39;00m\n",
      "\u001B[31mNameError\u001B[39m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "test_convolutional_layer = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1) # Defines our kernel for our first layer of convolution. \n",
    "test_max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "x = next(train_dataloader.__iter__())[0]\n",
    "print('Shape of x:', x.shape)\n",
    "## The shape will be 128, 1, 28, 28 because we have 128 images in a batch, 1 channel, and 28x28 pixels\n",
    "\n",
    "x2 = test_convolutional_layer(x)\n",
    "print('Shape of x2:', x2.shape)\n",
    "## The shape will be 128, 3, 26, 26 because we have 128 images in a batch, 5 channels, and 26x26 pixels\n",
    "\n",
    "x3 = test_max_pool(x2)\n",
    "print(\"shape of x3: \", x3.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-21T11:07:43.131156Z",
     "start_time": "2025-05-21T11:07:42.799792Z"
    }
   },
   "id": "3c50cea88c04c7d5",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bulding the Neural Net\n",
    "Here is were the different net classes (CNNs) are declared. We'll divide the architectures into 2 sets: A1 and A2.\n",
    "CNNs of A1 share the same architectures, but they differ in the initialization/training of the kernels, and the same happens for A2. \n",
    "We have 3 types of the initialization schemas:\n",
    "-  HF;\n",
    "- HT;\n",
    "- DT\n",
    "\n",
    "NB: WHEN YOU HAVE DECIDED TO USE THE DESIRED ARCHITECTURE, CHANGE THE NAME OF THE CLASS WITH \"Net\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bf6822b69b67a92"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SET A1\n",
    "### HF"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c3ab679a25b88d3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "#A1_HF\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1)  # Convolution layer\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(5 * 14 * 14, 100)  # first number is to decoding the 3d tensor vector into a 1D dimensional vector, 100 is the number of output neurons of fc1. I chosed 100 because is a good tradeoff between speed and leaning capacity\n",
    "        self.fc2 = nn.Linear(100, len(classes))  # Final fully connected layer. This is the layer that makes the preictions.\n",
    "        self.relu = nn.ReLU()  # Activation Function\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu(self.conv1(x)))\n",
    "\n",
    "        x = self.flatten(x)  # x becomes a 2D vector\n",
    "\n",
    "        x = self.relu(self.fc1(x))  # actv. function applied on the first fully connected layer of output\n",
    "\n",
    "        x = self.fc2(x)  # Prediction\n",
    "        return x\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f4cb37229cf3338",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### HT"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91362a8380f33776"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "#A1_HT\n",
    "class A1_HT(nn.Module):\n",
    "    def __init__(self, classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1)  # Convolution layer\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(5 * 14 * 14, 100)  # first number is to decoding the 3d tensor vector into a 1D dimensional vector, 100 is the number of output neurons of fc1. I chosed 100 because is a good tradeoff between speed and leaning capacity\n",
    "        self.fc2 = nn.Linear(100, len(classes))  # Final fully connected layer. This is the layer that makes the preictions.\n",
    "        self.relu = nn.ReLU()  # Activation Function\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def set_initial_kernels(self):\n",
    "        # Definito by hand i primi 5 kernels\n",
    "        kernel1 = torch.tensor([[0, 1, 0],\n",
    "                                [1, 1, 1],\n",
    "                                [0, 1, 0]], dtype=torch.float32)\n",
    "\n",
    "        kernel2 = torch.tensor([[1, 0, 1],\n",
    "                                [0, 1, 0],\n",
    "                                [1, 0, 1]], dtype=torch.float32)\n",
    "\n",
    "        kernel3 = torch.tensor([[1, 1, 1],\n",
    "                                [0, 0, 0],\n",
    "                                [1, 1, 1]], dtype=torch.float32)\n",
    "\n",
    "        kernel4 = torch.tensor([[0, 1, 0],\n",
    "                                [0, 0, 0],\n",
    "                                [1, 0, 1]], dtype=torch.float32)\n",
    "\n",
    "        kernel5 = torch.tensor([[1, 1, 0],\n",
    "                                [1, 0, 0],\n",
    "                                [0, 0, 0]], dtype=torch.float32)\n",
    "        kernels = [kernel1, kernel2, kernel3, kernel4, kernel5]  # list of\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # I pesi di conv1 hanno shape [5, 1, 3, 3] (5 kernels, 1 canale, dimensione 3x3)\n",
    "            # Assegno ciascun pattern al corrispondente kernel per l'unico canale in ingresso.\n",
    "            for k, kernel in enumerate(kernels):\n",
    "                self.conv1.weight[k, 0] = kernel\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu(self.conv1(x)))\n",
    "\n",
    "        x = self.flatten(x)  # x becomes a 2D vector\n",
    "\n",
    "        x = self.relu(self.fc1(x))  # actv. function applied on the first fully connected layer of output\n",
    "\n",
    "        x = self.fc2(x)  # Prediction\n",
    "        return x\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a009ebb15cc77b5",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### DT"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "325dfea149184acc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "#A1_DT\n",
    "class A1_DT(nn.Module):\n",
    "    def __init__(self, classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1)  # Convolution layer\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(5 * 14 * 14, 100)  # first number is to decoding the 3d tensor vector into a 1D dimensional vector, 100 is the number of output neurons of fc1. I chosed 100 because is a good tradeoff between speed and leaning capacity\n",
    "        self.fc2 = nn.Linear(100, len(classes))  # Final fully connected layer. This is the layer that makes the preictions.\n",
    "        self.relu = nn.ReLU()  # Activation Function\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu(self.conv1(x)))\n",
    "\n",
    "        x = self.flatten(x)  # x becomes a 2D vector\n",
    "\n",
    "        x = self.relu(self.fc1(x))  # actv. function applied on the first fully connected layer of output\n",
    "\n",
    "        x = self.fc2(x)  # Prediction\n",
    "        return x\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-21T11:07:43.133610Z",
     "start_time": "2025-05-21T11:07:43.133379Z"
    }
   },
   "id": "72e31f59f8435926",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set A2\n",
    "### HF"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f5304b84a525f5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "#A2_HF\n",
    "class A2_HF(nn.Module):\n",
    "    def __init__(self, classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1)  # Convolution layer\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(5 * 14 * 14, 100)  # first number is to decoding the 3d tensor vector into a 1D dimensional vector, 100 is the number of output neurons of fc1. I chosed 100 because is a good tradeoff between speed and leaning capacity\n",
    "        self.fc2 = nn.Linear(100, len(classes))  # Final fully connected layer. This is the layer that makes the preictions.\n",
    "        self.relu = nn.ReLU()  # Activation Function\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def set_initial_kernels(self):\n",
    "        # Definito by hand i primi 5 kernels\n",
    "        kernel1 = torch.tensor([[0, 1, 0],\n",
    "                                [1, 1, 1],\n",
    "                                [0, 1, 0]], dtype=torch.float32)\n",
    "\n",
    "        kernel2 = torch.tensor([[1, 0, 1],\n",
    "                                [0, 1, 0],\n",
    "                                [1, 0, 1]], dtype=torch.float32)\n",
    "\n",
    "        kernel3 = torch.tensor([[1, 1, 1],\n",
    "                                [0, 0, 0],\n",
    "                                [1, 1, 1]], dtype=torch.float32)\n",
    "\n",
    "        kernel4 = torch.tensor([[0, 1, 0],\n",
    "                                [0, 0, 0],\n",
    "                                [1, 0, 1]], dtype=torch.float32)\n",
    "\n",
    "        kernel5 = torch.tensor([[1, 1, 0],\n",
    "                                [1, 0, 0],\n",
    "                                [0, 0, 0]], dtype=torch.float32)\n",
    "        kernels = [kernel1, kernel2, kernel3, kernel4, kernel5]  # list of\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # I pesi di conv1 hanno shape [5, 1, 3, 3] (5 kernels, 1 canale, dimensione 3x3)\n",
    "            # Assegno ciascun pattern al corrispondente kernel per l'unico canale in ingresso.\n",
    "            for k, kernel in enumerate(kernels):\n",
    "                self.conv1.weight[k, 0] = kernel\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu(self.conv1(x)))\n",
    "\n",
    "        x = self.flatten(x)  # x becomes a 2D vector\n",
    "\n",
    "        x = self.relu(self.fc1(x))  # actv. function applied on the first fully connected layer of output\n",
    "\n",
    "        x = self.fc2(x)  # Prediction\n",
    "        return x\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-21T11:07:43.134229Z",
     "start_time": "2025-05-21T11:07:43.134071Z"
    }
   },
   "id": "7af36f698d71c15",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### HT"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "706682af836b399c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "# A2_HT\n",
    "class A2_HT(nn.Module):\n",
    "    def __init__(self, classes):\n",
    "        super(A2_HT, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1)  # Convolution layer\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(5 * 14 * 14, 100)  # first number is to decoding the 3d tensor vector into a 1D dimensional vector, 100 is the number of output neurons of fc1. I chosed 100 because is a good tradeoff between speed and leaning capacity\n",
    "        self.fc2 = nn.Linear(100, len(classes))  # Final fully connected layer. This is the layer that makes the preictions.\n",
    "        self.relu = nn.ReLU()  # Activation Function\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def set_initial_kernels(self):\n",
    "        # Definito by hand i primi 5 kernels\n",
    "        kernel1 = torch.tensor([[0, 1, 0],\n",
    "                                [1, 1, 1],\n",
    "                                [0, 1, 0]], dtype=torch.float32)\n",
    "\n",
    "        kernel2 = torch.tensor([[1, 0, 1],\n",
    "                                [0, 1, 0],\n",
    "                                [1, 0, 1]], dtype=torch.float32)\n",
    "\n",
    "        kernel3 = torch.tensor([[1, 1, 1],\n",
    "                                [0, 0, 0],\n",
    "                                [1, 1, 1]], dtype=torch.float32)\n",
    "\n",
    "        kernel4 = torch.tensor([[0, 1, 0],\n",
    "                                [0, 0, 0],\n",
    "                                [1, 0, 1]], dtype=torch.float32)\n",
    "\n",
    "        kernel5 = torch.tensor([[1, 1, 0],\n",
    "                                [1, 0, 0],\n",
    "                                [0, 0, 0]], dtype=torch.float32)\n",
    "\n",
    "        kernels = [kernel1, kernel2, kernel3, kernel4, kernel5]  # list of\n",
    "\n",
    "        for k, kernel in enumerate(kernels):\n",
    "            self.conv1.weight[k, 0] = kernel\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu(self.conv1(x)))\n",
    "\n",
    "        x = self.flatten(x)  # x becomes a 2D vector\n",
    "\n",
    "        x = self.relu(self.fc1(x))  # actv. function applied on the first fully connected layer of output\n",
    "\n",
    "        x = self.fc2(x)  # Prediction\n",
    "        return x\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-21T11:07:43.135182Z",
     "start_time": "2025-05-21T11:07:43.134861Z"
    }
   },
   "id": "f99dc1579004a940",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### DT"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77fb5cbb82f8c22f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "#A2_DT\n",
    "class A2_DT(nn.Module):\n",
    "    def __init__(self, classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1)  # Convolution layer\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(5 * 14 * 14, 100)  # first number is to decoding the 3d tensor vector into a 1D dimensional vector, 100 is the number of output neurons of fc1. I chosed 100 because is a good tradeoff between speed and leaning capacity\n",
    "        self.fc2 = nn.Linear(100, len(classes))  # Final fully connected layer. This is the layer that makes the preictions.\n",
    "        self.relu = nn.ReLU()  # Activation Function\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu(self.conv1(x)))\n",
    "\n",
    "        x = self.flatten(x)  # x becomes a 2D vector\n",
    "\n",
    "        x = self.relu(self.fc1(x))  # actv. function applied on the first fully connected layer of output\n",
    "\n",
    "        x = self.fc2(x)  # Prediction\n",
    "        return x\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-21T11:07:43.135893Z",
     "start_time": "2025-05-21T11:07:43.135626Z"
    }
   },
   "id": "811e2afef144425b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#net.to(device) # For having an idea about the structure of the net.\n",
    "# Start_dim = 1 for not flattening the batch size.\n",
    "# end_dim = -1 for flattening until the last dimension.\n",
    "# ex: (128, 5, 14, 14) --> (128, 980)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-21T11:07:43.136849Z",
     "start_time": "2025-05-21T11:07:43.136653Z"
    }
   },
   "id": "da4ae182f25061dc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#print(net.conv1.weight.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c883e1ad351903a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TRAIN_LOOP"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aed4db41fb8f1c5a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, epoch, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    print(f\"Training set of size: {size}\")\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):  # (X = input, y = target)\n",
    "        X, y = X.to(device), y.to(device)  # Setting of 2 architectures\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()  # Loss function calculating the zero-gradient descent\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 1000 == 0:  # every 1000 batch it prints the loss\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            current_loss = current / size\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    # torch save model with torch.save()\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,  # l'epoca corrente\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "\n",
    "    # Definisci il percorso della cartella dei checkpoint\n",
    "    checkpoint_dir = r'C:\\Users\\stefa\\Desktop\\DNN2025\\DNNStefano\\DNN\\CheckpointsNotebook'\n",
    "\n",
    "    # Se la cartella non esiste, la creiamo\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    # Costruiamo il percorso completo del file checkpoint\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'epoch_{epoch}_Model_CNN_A1_DT.pt')\n",
    "\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-21T11:07:43.138078Z",
     "start_time": "2025-05-21T11:07:43.138007Z"
    }
   },
   "id": "308601ce43acb109",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TEST_LOOP"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d971be3192c5050"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return 100 * correct, test_loss\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf5f7c710d718dd4",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# START"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8b75058fa3f6a1a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START A1_CNN_HF\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Training set of size: 60000\n",
      "loss: 2.386296  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.628638 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Training set of size: 60000\n",
      "loss: 0.496888  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.539108 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Training set of size: 60000\n",
      "loss: 0.433041  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.507820 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Training set of size: 60000\n",
      "loss: 0.472780  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.485265 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Training set of size: 60000\n",
      "loss: 0.338189  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.464642 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Training set of size: 60000\n",
      "loss: 0.402972  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.464683 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Training set of size: 60000\n",
      "loss: 0.291120  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.460363 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Training set of size: 60000\n",
      "loss: 0.443674  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.7%, Avg loss: 0.430884 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Training set of size: 60000\n",
      "loss: 0.386057  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.435548 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Training set of size: 60000\n",
      "loss: 0.399248  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.439180 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Training set of size: 60000\n",
      "loss: 0.324613  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.414249 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Training set of size: 60000\n",
      "loss: 0.352337  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.8%, Avg loss: 0.425043 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Training set of size: 60000\n",
      "loss: 0.392334  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.405126 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Training set of size: 60000\n",
      "loss: 0.454353  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.401169 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Training set of size: 60000\n",
      "loss: 0.503712  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.397535 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Training set of size: 60000\n",
      "loss: 0.392825  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.399655 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Training set of size: 60000\n",
      "loss: 0.357899  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.404390 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Training set of size: 60000\n",
      "loss: 0.323254  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.7%, Avg loss: 0.381391 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Training set of size: 60000\n",
      "loss: 0.321150  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.3%, Avg loss: 0.382166 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Training set of size: 60000\n",
      "loss: 0.365895  [  128/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.2%, Avg loss: 0.387931 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from DNNStefano.DNN.SetA1.models.CNN_HF.data_loader import handle_dataset\n",
    "from DNNStefano.DNN.SetA1.models.CNN_HF.test import test_loop\n",
    "from DNNStefano.DNN.SetA1.models.CNN_HF.train import train_loop\n",
    "from DNNStefano.DNN.SetA1.models.CNN_HF.CNN import Net\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def start(epochs, iteratore, device, train_loader, test_loader):\n",
    "    for iterator in range(iteratore, epochs):\n",
    "        print(f\"Epoch {iterator + 1}\\n-------------------------------\")\n",
    "        train_loop(train_dataloader, model, loss_fn, optimizer, iterator + 1, device)\n",
    "        accuracy, loss = test_loop(test_dataloader, model, loss_fn, device)\n",
    "        accuracies.append(accuracy)\n",
    "        losses.append(loss)\n",
    "    print(\"Done!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"START\")\n",
    "    train_dataloader, test_dataloader, labels_map = handle_dataset()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Net(labels_map)  # Dichiarazione di un oggetto di tipo Net\n",
    "    model.to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model.set_initial_kernels()\n",
    "    learning_rate = 1e-4\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    epochs = 20\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    iterator = 0\n",
    "    start(epochs, iterator, device, train_dataloader, test_dataloader)\n",
    "\n",
    "    epochs = range(1, len(accuracies) + 1)\n",
    "\n",
    "    # comment if it creates problems\n",
    "    matplotlib.use('TkAgg')  # Oppure 'Qt5Agg' se hai PyQt5 installato\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)  # Primo subplot\n",
    "    plt.plot(epochs, accuracies, marker='o', label=\"Accuracy\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'Accuracy Over Epochs')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)  # Secondo subplot\n",
    "    plt.plot(epochs, losses, marker='o', label=\"Loss\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Loss Over Epochs')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-21T11:09:26.516426Z",
     "start_time": "2025-05-21T11:07:58.109805Z"
    }
   },
   "id": "4eb03d2edcf00ba",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Per ripartire da un checkpoint se per qualche motivo si è bloccato l'addestramento"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfc5f32adbbddaa5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Sostituisci 'epoch_X_Model_CNN_A1_HF.pt' con il nome effettivo del file checkpoint che vuoi ispezionare\n",
    "checkpoint_path = r'Checkpoints\\epoch_3_Model_CNN_A1_HF.pt'\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# Ripristina lo stato del modello\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Ripristina lo stato dell'ottimizzatore\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Definisce da quale epoca ripartire\n",
    "restart = checkpoint['epoch']\n",
    "\n",
    "print(f\"Riprendo il training dall' epoca {restart}\")\n",
    "\n",
    "start(epochs, restart)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c58f7c718b9240c7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "75cb444a4d835fd2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9827150de3dd838d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
